{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"OVM"},{"location":"#ovm-observability-volume-manager","title":"OVM: Observability Volume Manager","text":"<p>In today's dynamic computing landscape spanning centralized clouds, multi-cloud, and edge computing, the demand for adaptable observability is paramount. In particular, managing the escalating volume of observability data is crucial. Observability Volume Manager addresses the complexities of dynamic observability data management by offering tailored processing capabilities to ensure efficient and effective observability across the entire computing infrastructure. </p>"},{"location":"#overview","title":"Overview","text":"<p>Observability Volume Manager provides a mechanism to perform various transformations on observability data to manage the volume and support various edge deployment use cases. - OVM minimizes compute, transport, and data persistency costs, while preserving the complete value of observability to detect, troubleshoot, and support analytics. - OVM dynamically responds to changes in system state and behavior, optimizing observability volume based on user-defined policies. - OVM encapsulates multiple layers of \"smartness\" and can be used in full automation mode or as a recommendation system supporting a \"user in the loop\" design. - OVM provides user-centric intent-based semantic language supporting multiple transformations represented as a DAG. This semantic language is deployment and technology-agnostic.</p> <p>The current version of OVM supports  OpenTelemetry  and   Prometheus as observability data collectors.  </p> <p>It uses either an embedded, lightweight, automated, SQL-based metric processing system  or the capabilities of the community OTel processors to manage and change the collected observability volume.      </p>"},{"location":"#architecture-a-birds-eye-view","title":"Architecture: A Bird's Eye View","text":"<p>The diagram above illustrates the system architecture of Observability Volume Manager. The system components i.e Processor, Manager, and Controller are deployed across the central cloud and edge locations:</p> <p>Central Cloud location   - The central cluster that is used to manage the edge clusters.   - Can be an on-premise or cloud-based cluster.   - There are no resources or network constraints expected in the core cluster.   - There is an observability data aggregator (such as Prometheus, Thanos, Loki, etc.) running on the central cluster.</p> <p>Single Cloud/Edge location   - This is an edge environment where the user workloads are running.   - The edge environment is expected to have moderate yet constrained compute resources and network.   - There are observability data collectors (such as OTel, Prometheus, Vector, etc.) running on the edge environment that are sending the data to the aggregator in the central cluster. </p> <p>Processor: Is deployed at the edge location. It intercepts the observability data collected by the collectors before being pushed to the aggregators in the central cloud. The Processor facilitates diverse transformations like filtering, aggregation, and granularity adjustments on the observability data.  Users can also apply combinations of these transformations, represented as a Directed Acyclic Graph (DAG).  More details on the processor can also be found in our paper.</p> <p>otel-collector-proxy is a new addition to OVM.  It is deployed at the edge location and acts as an intermediary layer between OVM and OTel Collector. It is responsible for translations and dynamically managing OTel collector processor configurations.</p> <p>Manager: is deployed at the central cloud location and is responsible for managing observability data transformations at the edge locations. It is a user-facing component with a REST interface to create/update/delete the rules that define the transformation DAGs to be enabled on a certain edge site(s) when some condition is met. The user can also specify default transformation DAGs for edge sites. The Manager coordinates with the processors to enforce transformation DAGs based on user-defined rules when conditions are satisfied. </p> <p>Controller: is deployed at the central cloud location.  It periodically analyzes the behavior of observability signals and correlates them with customer requirements and the usage of signals in the system. Utilizing this analysis, the Controller generates recommendations and can even automatically update rules in the manager to effectively manage and reduce observability data volume. </p> <p>Note: When OVM uses OTel processor capabilities in the otel-collector-proxy, the functionality is  limited to the supported transformations available by OTel, additional details can be  found on otel-collector-proxy/README.md and  on the OTel processors page</p>"},{"location":"#use-cases","title":"Use Cases","text":"<p>The main objective of the current version of Observability Volume Manager is to demonstrate centralized control and management of observability data across all connected edge environments from the core. Users can specify transformation DAGs for individual edge locations through rules, which define conditions based on metrics collected from these environments. </p> <p>The contrib folder holds various tools and scripts that helps to develop, build, and maintain the project. It also contains the docker compose to bring up all the components needed to run the proof of concept scenario locally.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>To demonstrate the value of OVM and provide a \"learning by example\" getting started experience, OVM provides several Proof of Concepts experiences. Each POC includes documentation and deployment of OVM and additional components demonstrating a specific scenario and exploiting OVM configuration.  The documentation provides clear, easy-to-follow steps for replicating the Proof of Concept on your setup (even on a local laptop).</p> <ul> <li>Rules-based POC</li> <li>Insights based POC</li> <li>OTel based POC</li> </ul> <p>Note: We have created a video demonstrating the Insights based POC in action.  It can be accessed here.</p> <p>Note: We have created a blog demonstrating the OTel based POC  It can be accessed here.</p>"},{"location":"#insights-based-proof-of-concept-walkthrough","title":"Insights based Proof of Concept Walkthrough","text":"<p>The diagram above illustrates our setup for the Insights based proof of concept. </p> <p>We have two edge locations each equipped with a metric generator, metric collector, and the OVM Processor. The metric generator produces app and cluster metrics, which are periodically scrapped by the metric collector and then transformed by the processor. The central cloud location runs the metric aggregator and the Manager. At the beginning of the proof of concept, the user specifies the two rules:</p> <ul> <li>Rule 1: Applies to Processor 1 and requests to filter (whitelist) only the app metrics when edge location 1 experiences network stress.</li> <li>Rule 2: Applies to Processor 2 and requests to increase the frequency of cluster metrics when edge location 2 encounters erroneous conditions.</li> </ul> <p>Apart from triggering transformations based on user defined rules,  we also have a brain (controller) component that aids users by suggesting rules for managing observability data. This controller possesses the capability to communicate with the manager to automatically configure these rules, streamlining the process and enhancing user experience. </p> <p>The proof of concept is built to showcase two main capabilities of the Observability Volume Manager.  1. Automation: Users only define rules for managing observability data under different conditions and the Observability Volume Manager autonomously and dynamically monitors and enforces these transformations without requiring user intervention each time conditions are met. This eliminates the need for user intervention each time conditions are met, alleviating the burden of configuring individual processors at edge locations everytime. 2. Specificity: The Observability Volume Manager can enforce a transformation DAG on a subset of Edge locations. Additionally, within these edge locations, specific transformations can be applied to designated metrics only. This fine-granied control provides a level of specificity lacking in current observability systems. 3. Intelligent Pruning: The controller analyzes the metrics to provide insights on similarity of metrics. This can be used as a recommendation or trigger pruning of metrics for intelligent metric transformation.</p> <p>More details on the proof of concept are explained here.</p>"},{"location":"contrib/ec-metric-gen/","title":"Metric Generation","text":"<p>GitHub reference: contrib/ec-metric-gen </p> <p>For local testing of the system we use a metric generator. The current version generates metrics in the prometheus formats comprising of a time series for each metric. Each metric is represented as set of labels and values.  The generator can be used to generate fake metrics as well as custom  metrics with user provided labels names. </p>"},{"location":"contrib/ec-metric-gen/#fake-metrics","title":"Fake Metrics","text":"<p>If the value of the metric name is not critical for the testing we can use the default mode to send the metrics. <pre><code>python3 gen-metrics-gutentag.py --fake --nmetrics 100 --nlabels 10\n</code></pre> You can vary the number of metrics and labels using the <code>--nmetrics</code> and <code>--nlabels</code> parameters respectively.</p>"},{"location":"contrib/ec-metric-gen/#with-a-conf-file","title":"With a conf file","text":"<p><pre><code>python3 gen-metrics-gutentag.py --conf conf.yaml --nmetrics 100 --nlabels 10\n</code></pre> The current conf supports certain set of metric type such as as can be seen in t</p>"},{"location":"contrib/ec-metric-gen/doc/metrics/","title":"Metrics","text":"<p>Flask App port = 5000</p> <p>Prometheus Client Port = 8000</p> <p>Sample CURL statements:</p> <ol> <li> <p>Set some metrics </p> <p>curl --location --request POST 'localhost:5000' \\ --header 'Content-Type: application/json' \\ --data-raw '{     \"type\": \"SET\",     \"cluster\": [     {     \"name\": \"1\",     \"clustermetrics\": [\"1\",\"2\",\"3\"],     \"node\": [         {         \"name\": \"1\",         \"nodemetrics\": [\"2\",\"3\"]         },         {         \"name\": \"2\",         \"nodemetrics\": [\"2\",\"7\"]         }     ],     \"app\": [         {         \"name\": \"A\",         \"appmetrics\": [\"3\",\"4\",\"5\"]         },         {         \"name\": \"B\",         \"appmetrics\": [\"1\",\"2\"]         }     ]     }     ] }'</p> </li> <li> <p>Reset the same metrics </p> <p>curl --location --request POST 'localhost:5000' \\ --header 'Content-Type: application/json' \\ --data-raw '{     \"type\": \"RESET\",     \"cluster\": [     {     \"name\": \"1\",     \"clustermetrics\": [\"1\",\"2\",\"3\"],     \"node\": [         {         \"name\": \"1\",         \"nodemetrics\": [\"2\",\"3\"]         },         {         \"name\": \"2\",         \"nodemetrics\": [\"2\",\"7\"]         }     ],     \"app\": [         {         \"name\": \"A\",         \"appmetrics\": [\"3\",\"4\",\"5\"]         },         {         \"name\": \"B\",         \"appmetrics\": [\"1\",\"2\"]         }     ]     }     ] }'</p> </li> <li> <p>Set all metrics</p> <p>curl --location --request POST 'localhost:5000' \\ --header 'Content-Type: application/json' \\ --data-raw '{     \"type\": \"SET ALL\" }'</p> </li> <li> <p>Reset all metrics</p> <p>curl --location --request POST 'localhost:5000' \\ --header 'Content-Type: application/json' \\ --data-raw '{     \"type\": \"RESET ALL\" }'</p> </li> </ol>"},{"location":"contrib/end2end/","title":"End-To-End tests","text":"<p>GitHub reference: contrib/end2end </p> <p>This directory holds scripts and configuration for end-to-end tests of the observability volume controller. The end-to-end tests are based upon local set-ups of two use cases:</p> <ol> <li>prometheus at the edges and thanos at the hub </li> <li>oTel collector at the edges and prometheus at the hub</li> </ol>"},{"location":"contrib/end2end/#use-case-1-prometheus-at-the-edges-and-thanos-at-the-hub","title":"Use case (1): prometheus at the edges and thanos at the hub","text":"<p>To manually start e2e containers, use  <code>make up-prom-thanos</code> To shut down the containers, use: <code>make down-prom-thanos</code></p> <p>The setup automatically starts the following containers:  </p>"},{"location":"contrib/end2end/#what-does-it-start","title":"What does it start?","text":"Service Ports prometheus_one The first Prometheus server 9001 prometheus_two The second Prometheus server 9002 minio A minio instance serving as Object Storage for store, compactor and sidecars 9000 minio-console user: myaccesskey password: mysecretkey 9007 thanos_sidecar_one First Thanos sidecar for prometheus_one thanos_sidecar_two Second Thanos sidecar for prometheus_two thanos_querier Thanos querier instance connected to both sidecars and Thanos store 10902 thanos_query_frontend Thanos query frontend connected to querier 19090 thanos_store A Thanos store instance connected to minio 10912"},{"location":"contrib/end2end/#query-examples","title":"query examples","text":"<p>For example, to get the  <code>go_memstats_frees_total</code> metric values  from time stamp <code>1711895548</code> to timestamp <code>171189954</code> in steps of <code>560s</code> execute the query:</p> <pre><code>curl 'http://localhost:19090/api/v1/query_range?query=go_memstats_frees_total&amp;start=1711895548&amp;end=1711899548&amp;step=560s' | jq\n</code></pre>"},{"location":"contrib/end2end/#use-case-2-otel-collector-at-the-edges-and-prometheus-at-the-hub","title":"Use case (2): oTel collector at the edges and prometheus at the hub","text":"<p>To manually start e2e containers, use  <code>make up-otel-prom</code> To shut down the containers, use: <code>make down-otel-prom</code></p> <p>The setup automatically starts the following containers:  </p>"},{"location":"contrib/end2end/#what-does-it-start_1","title":"What does it start?","text":"Service Ports prometheus_hub A Prometheus instance 9001 otel_collector_one The first otel collector instance 18888 otel_collector_two The second collector instance 18889"},{"location":"contrib/end2end/#query-examples_1","title":"query examples","text":"<p>For example, to get the  <code>go_memstats_frees_total</code> metric values  from time stamp <code>1711895548</code> to timestamp <code>171189954</code> in steps of <code>560s</code> execute the query:</p> <pre><code>curl 'http://localhost:9001/api/v1/query_range?query=system_cpu_time_seconds_total&amp;start=1711895548&amp;end=1711899548&amp;step=560s' | jq\n</code></pre>"},{"location":"contrib/end2end/poc/insight_based_poc/","title":"PoC usecase","text":"<p>The PoC demonstrates two edges connected to a central cloud. Each edge comprises of metric generator whose metrics are scraped by prometheus. The prometheus does a remote write to thanos (running in the central cloud) for long term storage and analysis. The remote write is intercepted by our processor proxy running at each edge location. The processor applies various transformation to the collected metrics.</p> <p>In this PoC we showcase how our controller generate insights that trigger pruning of metrics based on similarity across east and west edge/clouds.</p>"},{"location":"contrib/end2end/poc/insight_based_poc/#environment-setup","title":"Environment Setup","text":"<p>The PoC requires docker installed on the machine to test the scenario. The following containers get installed when the PoC environment is bought up.\\ Central containers: - <code>thanos-receive</code> - <code>thanos-query</code> - <code>thanos-ruler</code> - <code>ruler-config</code> - <code>alertmanager</code> - <code>controller</code> - <code>manager</code>\\ Edge containers: - <code>metricgen1,2</code> - <code>prometheus1,2</code> - <code>pmf_processsor1,2</code></p>"},{"location":"contrib/end2end/poc/insight_based_poc/#running-the-poc-story","title":"Running the PoC story","text":"<ol> <li>Bring up the PoC environment <pre><code>make start\n</code></pre> or  <pre><code>docker-compose -f docker-compose-quay.yml up -d\n</code></pre></li> <li>Confirm that the metrics are available in <code>thanos query</code> UI <code>http://127.0.0.1:19192</code>:    </li> </ol> <p>The following metrics should be available: <code>k8s_pod_network_bytes or nwdaf_5G_network_utilization</code>.</p> <p>There should be a single metric for each edge, hence a total of four metrics.</p> <ol> <li> <p>Next, we trigger the controller to draw insights <pre><code>make perform_analysis\n</code></pre> or via the manager API.  <code>http://127.0.0.1:5010/apidocs/#/Controller/post_api_v1_analyze</code></p> </li> <li> <p>Verify the generated insights in the Controller UI: <code>http://127.0.0.1:5000/insights</code> Click <code>Insights details 3</code> and the analysis should show three metrics: <code>k8s_pod_network_bytes($app,c0,metricgen2:8001,west,$IP)</code> <code>nwdaf_5G_network_utilization(analytic_function,c0,metricgen1:8000,east,$IP)</code> <code>nwdaf_5G_network_utilization(analytic_function,c0,metricgen2:8001,west,$IP)</code>  Those metrics are analyzed as correlated with <code>k8s_pod_network_bytes</code>.</p> </li> <li> <p>The insights can be used just as a recommendation or for full-automation where corresponding transform will be applied to each edge to handle the pruning.  In this POC we demonstrate the full automation mode.</p> </li> <li> <p>The controller triggers transformation in each cloud which can be seen using the manager UI:    Access <code>http://127.0.0.1:5010/apidocs/#/Processor%20Configuration/getProcessorConfig</code>.   Use <code>east</code> and <code>west</code> in the UI combo box as the processor ids to check transformation added to individual clouds.   </p> </li> <li> <p>You can visualize the change in the metrics values:  Execute the query <code>k8s_pod_network_bytes or nwdaf_5G_network_utilization</code>) in the <code>thanos query</code> UI (in graph mode).  You will see only <code>k8s_pod_network_bytes</code> with label <code>processor=\"east\"</code> flowing. Other analyzed as similar metrics are now being pruned/dropped.</p> </li> <li> <p>To end the POC and clean docker execute:   <pre><code>make end\n</code></pre> or  <pre><code>docker-compose -f docker-compose-quay.yml down\n</code></pre></p> </li> </ol>"},{"location":"contrib/end2end/poc/otel_based_poc/","title":"PoC use-case","text":"<p>The PoC demonstrates two edges connected to a central cloud.  Each edge comprises a metric generator whose metrics are scraped by an OTel collector.  The collector does a remote write to Thanos (running in the central cloud) for long-term storage and analysis.  The remote write is intercepted as part of the OTel processor pipeline workflow.  A \"configuration proxy\" is used to apply various transformations from the central cloud  to the processors inside the OTel collectors. </p> <p>This PoC uses OTel collectors and showcases how the controller generates insights that trigger metrics pruning based on similarity across east and west edges/clouds.</p>"},{"location":"contrib/end2end/poc/otel_based_poc/#environment-setup","title":"Environment Setup","text":"<p>The PoC requires docker installed on the machine to test the scenario.  The following containers get installed when the PoC environment is bought up.</p> <p>Central containers: - <code>thanos-receive</code> - <code>thanos-query</code> - <code>thanos-ruler</code> - <code>ruler-config</code> - <code>alertmanager</code> - <code>controller</code> - <code>manager</code></p> <p>Edge containers: - <code>metricgen_[east,west]</code> - <code>otel_proxy_collector_[east,west]</code> - <code>otel_collector_[east,west]</code></p>"},{"location":"contrib/end2end/poc/otel_based_poc/#running-the-poc-story","title":"Running the PoC story","text":"<ol> <li>Bring up the PoC environment <pre><code>make start\n</code></pre> or  <pre><code>docker-compose -f docker-compose-otel.yml up -d\n</code></pre></li> <li>Confirm that the metrics are flowing correctly in the <code>thanos query</code> UI: <code>http://127.0.0.1:19192</code> Confirm metrics <code>k8s_pod_network_bytes</code> or <code>nwdaf_5G_network_utilization</code> are available. There should be a single metric for each edge, hence a total of four metrics.</li> <li>Next, we trigger the controller to draw insights <pre><code>make perform_analysis\n</code></pre> or Via the manager API. <code>http://127.0.0.1:5010/apidocs/#/Controller/post_api_v1_analyze</code></li> <li>Verify the generated insights in the Controller UI: <code>http://127.0.0.1:5000/insights</code> Click <code>Insights details 4</code> and the analysis should show three metrics (<code>k8s_pod_network_bytes($app,c0,metricgen2:8001,west,$IP),nwdaf_5G_network_utilization(analytic_function,c0,metricgen1:8000,east,$IP),nwdaf_5G_network_utilization(analytic_function,c0,metricgen2:8001,west,$IP)</code>) that are correlated with <code>k8s_pod_network_bytes</code>.</li> <li>The insights can be used just as a recommendation or for full automation, where a corresponding transform will be applied to each edge to handle the pruning. In this post, we will run in full automation mode. </li> <li>The controller triggers transformation in each cloud which can be seen using the manager UI  Access <code>http://127.0.0.1:5010/apidocs/#/Processor%20Configuration/getProcessorConfig</code>. Use <code>east</code> and <code>west</code> in the UI combo box as the processor ids to check the transformation added to individual clouds. </li> <li>You can visualize the change in the metrics values  Execute the query <code>k8s_pod_network_bytes or nwdaf_5G_network_utilization</code>) in the <code>thanos query</code> UI (in graph mode).  You will see only <code>k8s_pod_network_bytes</code> with label <code>processor=\"east\"</code> flowing. Other analyzed as similar metrics are now being pruned/dropped.</li> <li>To end the POC and clean docker execute <pre><code>make end\n</code></pre> or  <pre><code>docker compose -f docker-compose-otel.yml down\n</code></pre></li> </ol>"},{"location":"contrib/end2end/poc/otel_based_poc_interval/","title":"PoC use-case","text":"<p>The PoC demonstrates two edges connected to a central cloud.  Each edge comprises a metric generator whose metrics are scraped by an OTel collector.  The collector does a remote write to Thanos (running in the central cloud) for long-term storage and analysis.  The remote write is intercepted as part of the OTel processor pipeline workflow.  A \"configuration proxy\" is used to apply various transformations from the central cloud  to the processors inside the OTel collectors. </p> <p>This PoC uses a customized OTel collector and showcases how the controller generates insights that trigger adjusting frequency of metrics collection.</p>"},{"location":"contrib/end2end/poc/otel_based_poc_interval/#environment-setup","title":"Environment Setup","text":"<p>The PoC requires docker installed on the machine to test the scenario.  The following containers get installed when the PoC environment is bought up.</p> <p>Central containers: - <code>thanos-receive</code> - <code>thanos-query</code> - <code>thanos-ruler</code> - <code>ruler-config</code> - <code>alertmanager</code> - <code>controller</code> - <code>manager</code></p> <p>Edge containers: - <code>metricgen_[east,west]</code> - <code>otel_proxy_collector_[east,west]</code> - <code>otel_collector_[east,west]</code></p>"},{"location":"contrib/end2end/poc/otel_based_poc_interval/#running-the-poc-story","title":"Running the PoC story","text":"<ol> <li>Bring up the PoC environment <pre><code>make start\n</code></pre> or  <pre><code>docker-compose -f docker-compose-otel.yml up -d\n</code></pre></li> <li>Confirm that the metrics are flowing correctly in the <code>thanos query</code> UI: <code>http://127.0.0.1:19192</code> Confirm metrics <code>k8s_pod_network_bytes</code> or <code>nwdaf_5G_network_utilization</code> are available. Confirm metrics <code>process_cpu_seconds_total</code> are available and are collected at 5 second intervals. Let the demo run until the <code>k8s_pod_network_bytes</code> metric is no longer monotone.</li> <li>Next, we trigger the controller to draw insights <pre><code>make perform_analysis\n</code></pre> or Via the manager API. <code>http://127.0.0.1:5010/apidocs/#/Controller/post_api_v1_analyze</code></li> <li>Verify the generated insights in the Controller UI: <code>http://127.0.0.1:5000/insights</code> Click <code>Insights details 3</code> and the analysis should show the list of metrics identified as monotone. This should include the metric <code>process_cpu_seconds_total</code>.</li> <li>The insights can be used just as a recommendation or for full automation, where a corresponding transform will be applied to each edge to handle the pruning. In this post, we will run in full automation mode. </li> <li>The controller triggers transformation in each cloud which can be seen using the manager UI  Access <code>http://127.0.0.1:5010/apidocs/#/Processor%20Configuration/getProcessorConfig</code>. Use <code>east</code> and <code>west</code> in the UI combo box as the processor ids to check the transformation added to individual clouds. </li> <li>You can visualize the change in the metrics values  Execute the query <code>process_cpu_seconds_total</code> in the <code>thanos query</code> UI (in graph mode).  You will see that the frequency of the update of the metric has changed from 5 seconds to 30 seconds. Execute the query <code>nwdaf_5G_network_utilization</code> in the <code>thanos query</code> UI (in graph mode).  You will see that the frequency of the update of the metric remains unchanged at 5 seconds. Execute the query <code>k8s_pod_network_bytes</code> in the <code>thanos query</code> UI (in graph mode).  You will see that the frequency of the update of the metric has changed from 5 seconds to 20 seconds. These frequency definitions can be seen at the end of the file contrib/end2end/poc/otel_based_poc_interval/controller/config/config.yaml.</li> <li>To end the POC and clean docker execute <pre><code>make end\n</code></pre> or  <pre><code>docker compose -f docker-compose-otel.yml down\n</code></pre></li> </ol>"},{"location":"contrib/end2end/poc/rule_based_poc/","title":"PoC usecase","text":"<p>The PoC demonstrates two edges connected to a central cloud. Each edge comprises of metric generator whose metrics are scraped by prometheus. The prometheus does a remote write to thanos (running in the central cloud) for long term storage and analysis. The remote write is intercepted by our processor proxy running at each edge location. The processor applies various transformation to the collected metrics.</p> <p>In this PoC we showcase two transformations. - On east cloud/edge we showcase the need to control the bandwidth utilization. We identify the increase in bandwidth utilization using alerts and our manager triggers addition of the filter transformation on processor running on east cloud/edge. This will limit the volume of data sent from east edge.  - On west cloud/edge, we showcase the need to change the frequency of metric collection for a particular app/cluster. This scenario arises when for example an issue is identified at a node in a cluster and you need metrics for that node at a higher frequency to fix this issue. The identification of the issue in the PoC is also done with help of alerts on the metrics. </p> <p>Both the PoC scenarios which can also happen simulatenously, we also showcase how we revert the applied transformation once the issue is resolved. We instrument the issue with increase in metric values and reduce the value to revert the issue. </p>"},{"location":"contrib/end2end/poc/rule_based_poc/#environment-setup","title":"Environment Setup","text":"<p>The PoC requires docker installed on the machine to test the scenario. The following containers get installed when the PoC environment is bought up.\\ Central containers: - <code>thanos-receive</code> - <code>thanos-query</code> - <code>thanos-ruler</code> - <code>ruler-config</code> - <code>alertmanager</code> - <code>manager</code>\\ Edge containers: - <code>metricgen1,2</code> - <code>prometheus1,2</code> - <code>pmf_processsor1,2</code></p>"},{"location":"contrib/end2end/poc/rule_based_poc/#understanding-the-rules-specification","title":"Understanding the Rules specification","text":"<p>The rules are specified by the user in a yaml format. The manager parses them and applies them to the thanos ruler with the help of the <code>ruler_config</code> container. The rules file that the user specifies as part of the demo is <code>demo_2rules.yaml</code>. As per the file we apply two rules: - First rule <code>rule_1</code> is applied for metrics received from east edge. The rule is triggered when the metric <code>app_A_network_metric_0{IP=\"192.168.1.3\"}</code> cross the value of 200. The specification also mentions what needs to be done when the alert is triggered (<code>firing_action</code>). In the demo for this cloud, we add a new transform to filter all other metrics and allow just <code>app_A_network_metric_0</code> for IP 192.168.1.3 from cloud east. The <code>resolved_action</code> specifies what to be done when the alert is resolved. Here, we simply remove the added transform. - Second rule <code>rule_2</code> is applied for metics received from west edge. The rule is triggered when the metric <code>cluster_hardware_metric_0{node=\"0\"}</code> cross the value of 200. The <code>firing_action</code> is to increase the frequency of <code>cluster_hardware_metric_0</code> for <code>node:0</code> to 5 seconds.</p> <p>Note that the actions (firing and resolved) are stored by the manager and applied to the corresponding edge processor when the alert is fired/resolved.</p>"},{"location":"contrib/end2end/poc/rule_based_poc/#running-the-poc-story","title":"Running the PoC story","text":"<ol> <li>Bring up the PoC environment <pre><code>docker-compose -f docker-compose-quay.yml up -d\n</code></pre></li> <li>Add the rules and actions (in the form of transformation) corresponding to the rule <pre><code>curl -X POST --data-binary @demo_2rules.yaml -H \"Content-type: text/x-yaml\" http://0.0.0.0:5010/api/v1/rules\n</code></pre></li> <li>Confirm the two rules are added correctly in the <code>thanos ruler</code> UI: <code>http://0.0.0.0:10903/rules</code></li> <li>Confirm the metrics are flowing correctly in the <code>thanos query</code> UI: <code>http://0.0.0.0:19192/</code>\\ Search for <code>cluster_hardware_metric_0</code> and <code>app_A_network_metric_0</code>. You should see 6 metrics (3 for each edge) for each of these. The edge can be identified by the <code>processor</code> label in the metric.</li> <li>Next, we instrument the issue scenario with metric value change. We apply the value change to metricgen for each edges (<code>metricgen1</code> and <code>metricgen2</code>)<ul> <li>For issue at east edge: <code>curl -X POST --data-binary @change_appmetric.yml -H \"Content-type: text/x-yaml\" http://0.0.0.0:5002</code></li> <li>For issue at west edge: <code>curl -X POST --data-binary @change_hwmetric.yml -H \"Content-type: text/x-yaml\" http://0.0.0.0:5003</code></li> </ul> </li> <li>You can visualize the change in the metrics value in the <code>thanos query</code> UI (in the graph mode)</li> <li>The alert should also be firing and can be seen in the <code>thanos ruler</code> UI (<code>http://0.0.0.0:10903/alerts</code>)    P.S. Wait for 30sec-1min before checking this step. </li> <li>The manager triggers adding of the corresponding transforms. To check the transforms added got to Manager API: <code>http://0.0.0.0:5010/apidocs/#/Processor%20Configuration/getProcessorConfig</code> and use <code>east</code> and <code>west</code> as processor id.\\    P.S. This step is just for extra confirmation and is optional. </li> <li>To visualize the transformation thanos query UI (in the graph mode).<ul> <li>If you search <code>app_A_network_metric_0{processor=\"east\"}</code> you will see the metric with label <code>IP:192.168.1.3</code> has changing value but the other metrics with label <code>IP:192.168.1.1</code> and <code>IP:192.168.1.2</code> with no change (straight line; showcasing no value change or should stop completely).\\ This is because we have filtered and allowed <code>app_A_network_metric_0</code> only for app with <code>IP:192.168.1.3</code>.</li> <li>If you search <code>cluster_hardware_metric_0{processor=\"west\"}</code> you will see the metric with label <code>node:'0'</code> having a nice sinusoidal wave. This is because its value is changing every 5 sec. The other metrics with label <code>node:'1'</code> and <code>node:'2'</code> will be blocky showing their frequency is still 30 sec.  This showcases the transformation happening in an automated fashion.</li> </ul> </li> <li> <p>We will just revert issue in edge cloud east and showcase that we work on specific cloud as well.     <pre><code>curl -X POST --data-binary @change_appmetricRESET.yml -H \"Content-type: text/x-yaml\" http://0.0.0.0:5002\n</code></pre>     In sometime, you should see the alert rule 1 resolved (in thanos ruler UI (<code>http://0.0.0.0:10903/alerts</code>)) and should see the metric <code>app_A_network_metric_0{processor=\"east\"}</code> again coming in for labels <code>IP:192.168.1.1</code> and <code>IP:192.168.1.2</code> as well demontrating the removal of <code>filter</code> transform from edge cloud 1. </p> </li> <li> <p>P.S. One can revert the transformation applied to west edge as well using the below command.     <pre><code>curl -X POST --data-binary @change_hwmetricRESET.yml -H \"Content-type: text/x-yaml\" http://0.0.0.0:5003\n</code></pre></p> </li> <li>Note: The PoC can also be tested with using OTel Collector instead of prometheus. For this the only step that will change is -  1. Bring up the PoC environment to below     <pre><code>docker-compose -f docker-compose-otel.yml up -d\n</code></pre></li> </ol>"},{"location":"contrib/examples/generate-synthetic-metrics/","title":"generate-synthetic-metrics","text":"<p>GitHub reference: contrib/examples/generate-synthetic-metrics </p> <p>This folder holds code that generates a fixed static set of metrics in prometheus API format. The code generates a json file holding the metrics. The file name is hard-coded <code>time_series_data.json</code>. That file can be ingested into the controller component using <code>file ingest</code>. For additional details, access the controller component documentation  </p> <p></p> <p>The metrics, as shown above, are used for basic validation of the controller functionality based on simple predictable synthetic data.  </p> <p>Execute using:  </p> <pre><code>python generate-synthetic-metrics.py\n</code></pre> <p>Note: To visualize the metrics (in a new window using pyplot) use <code>--plot</code> at the command line  </p>"},{"location":"contrib/fetch-offline-data/instana/","title":"Instana Observability Data Fetchers","text":"<p>GitHub reference: fetch-offline-data/instana </p> <p>Those Python scripts allow fetching metrics, events, traces and topology data from Instana using REST API. The scripts persist the data in JSON format into files.</p>"},{"location":"contrib/fetch-offline-data/instana/#motivation","title":"Motivation","text":"<p>Instana is a popular monitoring tool used by many organizations to monitor their infrastructure and applications. This script provides a convenient way to programmatically fetch metrics and events data from Instana, allowing users to analyze and process the data as needed.</p>"},{"location":"contrib/fetch-offline-data/instana/#usage","title":"Usage","text":"<ol> <li> <p>Installation: Before running the scripts, make sure you have Python installed on your system. You can download and install Python from python.org.</p> </li> <li> <p>Dependencies: Install the required Python packages using pip:</p> </li> </ol> <pre><code>pip install requests urllib3\n</code></pre> <ol> <li>Configuration: You need to obtain an Instana API token and know the base URL of your Instana instance. Set these values in the script or provide them as command-line arguments (see below).</li> <li>Execute the Scripts: Use the following commands to run the relevant scripts:</li> </ol>"},{"location":"contrib/fetch-offline-data/instana/#instana-infrastructure-metrics","title":"INSTANA Infrastructure metrics","text":""},{"location":"contrib/fetch-offline-data/instana/#args","title":"Args","text":"<ol> <li><code>--log-level</code> - [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]</li> <li><code>--start</code> - start date in YYYY-MM-DDTHH:MM:SS format</li> <li><code>--end</code> - end date in YYYY-MM-DDTHH:MM:SS format</li> <li><code>--url</code> - Instana instance bse url</li> <li><code>--token</code> - List of tokens (tokens will be rotated in api calls to prevent api limit error)</li> <li><code>--output_dir</code> - Folder to store retrieved data</li> <li><code>--plugin</code> - Filter data by plugin. Eg: prometheus</li> <li><code>--query</code> - Filter data by query. Eg: namespace=kube</li> </ol>"},{"location":"contrib/fetch-offline-data/instana/#example","title":"Example","text":"<ul> <li>Fetching past 1 day data: <pre><code>python fetch_instana_data.py --url https://wmlpreprod-ibmdataaiap.instana.io --token EN6NUPBATJit5SJI_**** --query namespace=watsonx-huggingface --plugin prometheus --start 2024-05-28T00:00:00 --end 2024-05-29T00:00:00 --output_dir data\n</code></pre></li> <li>Fetching past 20 minutes: <pre><code>python fetch_instana_data.py --url https://wmlpreprod-ibmdataaiap.instana.io --token EN6NUPBATJit5SJI_**** --query namespace=watsonx-huggingface --plugin prometheus --start 2024-05-28T23:40:00 --end 2024-05-29T00:00:00 --output_dir data\n</code></pre></li> <li>Using default start time and end time with multiple tokens: <pre><code>python fetch-instana-data.py --url https://wmlpreprod-ibmdataaiap.instana.io --token EN6NUPBATJit5SJI_**** JSY9IUWvQEapPBZ4yC**** --output_dir data\n</code></pre></li> </ul>"},{"location":"contrib/fetch-offline-data/instana/#instana-application-metrics","title":"INSTANA Application metrics","text":""},{"location":"contrib/fetch-offline-data/instana/#args_1","title":"Args","text":"<ol> <li><code>--log-level</code> - [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]</li> <li><code>--start</code> - start date in YYYY-MM-DDTHH:MM:SS format</li> <li><code>--end</code> - end date in YYYY-MM-DDTHH:MM:SS format</li> <li><code>--url</code> - Instana instance bse url</li> <li><code>--token</code> - List of tokens (tokens will be rotated in api calls to prevent api limit error)</li> <li><code>--output_dir</code> - Folder to store retrieved data</li> </ol>"},{"location":"contrib/fetch-offline-data/instana/#example_1","title":"Example","text":"<ul> <li>Getting application metrics: <pre><code>python fetch_instana_app_metrics.py --url https://instana1.tivlab.raleigh.ibm.com --token tmdOVgSMS7ucB2r**** y7l5tD1wR9G581LY**** tMd8pw4WS6SdLkx**  --start 2024-07-02T00:00:00 --end 2024-07-03T00:00:00 --output_dir data\n</code></pre></li> </ul>"},{"location":"contrib/fetch-offline-data/instana/#instana-application-topology","title":"INSTANA Application topology","text":""},{"location":"contrib/fetch-offline-data/instana/#args_2","title":"Args","text":"<ol> <li><code>--log-level</code> - [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]</li> <li><code>--start</code> - start date in YYYY-MM-DDTHH:MM:SS format</li> <li><code>--end</code> - end date in YYYY-MM-DDTHH:MM:SS format</li> <li><code>--url</code> - Instana instance bse url</li> <li><code>--token</code> - Instana api token (string)</li> <li><code>--output_dir</code> - Folder to store retrieved data</li> </ol>"},{"location":"contrib/fetch-offline-data/instana/#example_2","title":"Example","text":"<ul> <li>Getting application topology: <pre><code>python fetch_instana_traces.py --traces_limit 10 --service_name_filter aggregator --url https://blue-instanaops.instana.io --token $apiToken --output_dir demo-eu\n</code></pre></li> </ul>"},{"location":"contrib/fetch-offline-data/instana/#instana-traces","title":"INSTANA traces","text":""},{"location":"contrib/fetch-offline-data/instana/#args_3","title":"Args","text":"<ol> <li><code>--log-level</code> - [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"]</li> <li><code>--start</code> - start date in YYYY-MM-DDTHH:MM:SS format</li> <li><code>--end</code> - end date in YYYY-MM-DDTHH:MM:SS format</li> <li><code>--url</code> - Instana instance bse url</li> <li><code>--token</code> - Instana api token (string)</li> <li><code>--service_name_filter</code> - Service name filter for traces</li> <li><code>--traces_limit</code> - Maximum number of traces to dump</li> <li><code>--output_dir</code> - Folder to store retrieved data</li> </ol>"},{"location":"contrib/fetch-offline-data/instana/#example_3","title":"Example","text":"<ul> <li>Getting traces, limiting to 10 traces: <pre><code>python fetch_instana_traces.py --url https://instana1.tivlab.raleigh.ibm.com --token y7l5tD1wR9G581LYO**** --traces_limit 10 --output_dir data\n</code></pre></li> </ul> <p>[!NOTE] Replace YOUR_INSTANA_URL and YOUR_API_TOKEN with the base URL of your Instana instance and your API token, respectively.</p> <p>You can also specify the START_TIME and END_TIME in the format YYYY-MM-DDTHH:MM:SS to fetch data for a specific time window. If you omit the --start and --end parameters, the script will fetch data for the last 24 hours by default.</p> <p>You can also specify a QUERY string to limit fetching to subset of the instana snapshots If you omit the --query parameters, the script will fetch data from all snapshots.</p> <p>You can also specify a PLUGINS_FILTER string to limit fetching to regex instana plugins If you omit the --plugins_filter parameters, the script will fetch data from all plugins.</p> <p>You can also specify the OUTPUT_DIR to save the data into a specific directory. If you omit the --output_dir parameter, the script will save the data into the current directory.  </p> <p>You can also specify the LIMIT to limit data fetching. If you omit the --limit parameter, the script will limit to 1000 metrics.</p> <p>By default, the script fetches metrics only, specify --fetch-events to include also events (Not implemented yet)  </p> <ol> <li>Output: The script will save the fetched metrics and events data to JSON files named instana_metrics.json and instana_events.json, respectively.</li> </ol> <p>[!NOTE] It is possible to execute the script <code>convert-dump-metrics-file-to-promql-format.py</code>  as a post-processing stage, and convert output metrics files into promQL format dump file. This file can be used with the  as input to the volume manager controller.  </p> <p>The default input file for the script is <code>instana_metrics.json</code> and the default output file is <code>promql_metrics.json</code> </p> <p>To load into the controller, execute from the controller directory:</p> <pre><code>python main.py --ingest_type=file --ingest_file=../contrib/fetch-offline-data/instana/promql_metrics.json --feature_extraction_type=tsfel --config_generator_type=otel --config_generator_directory=/tmp\n</code></pre> <p>[!NOTE] It is possible to execute the script <code>parse-instana-traces-queries-into-access-log.py</code> as a ost-processing stage, and convert output traces files into access_log format as input to the volume manager controller.</p> <p>Example usage: <pre><code>python parse-instana-traces-queries-into-access-log.py --input_file demo-eu/traces.json --output_dir demo-eu\n</code></pre></p>"},{"location":"controller/","title":"Controller","text":"<p>The controller is tasked with overseeing the management of the volume for observability data across the processors and collectors. It does this by analyzing the behavior of observability signals and correlating them with customer requirements and the usage of signals in the system. Based on this analysis, the controller generates recommendations and automatically updates configurations to manage and reduce the volume of observability data. </p>"},{"location":"controller/#architecture","title":"architecture","text":"<p>The controller architecture is described at docs/architecture.md</p>"},{"location":"controller/#config","title":"config","text":"<p>The controller can be configured to run with different stages and parameters. See docs/config.md.</p>"},{"location":"controller/#how-to-run","title":"How to run","text":"<p>To run the code, use the provided Makefile.</p> <p>For detailed execution options, use <code>make help</code></p>"},{"location":"controller/docs/architecture/","title":"Volume Manager Controller","text":""},{"location":"controller/docs/architecture/#architecture","title":"Architecture","text":"<p>The Volume Manager Controller is responsible for receiving information from the Observability Collectors and Observability Processors. It then performs a set of operations to contextualize and analyze this information. Along with the user-provided policies, the Controller generates insights and conducts automation operations to control and optimize the volume of observability data.</p> <p>The volume manager is built around pipelines of tasks that are executed asynchronously. Each task performs a different function, and the pipelines are configurable to allow flexibility and support for various use cases.</p> <p>Tasks can be optimized and parallelized to enhance scalability and efficiency. Each task is classified based on its type, which determines its specific functionality and characteristics. The input and output data structures of each task differ from one another. The controller manages and arranges the tasks in Directed Acyclic Graphs (DAGs), and performs operations on them periodically and on-demand. The data ingestion into the controller is batched and processed between the various task operations.</p> <p>The controller requires data persistence to efficiently analyze observability data over time. Data persistence is decoupled from computing tasks, allowing flexibility for specific use cases.</p> <p>The controller's behavior is governed by a collection of high-level semantic policies that are visible to the user. This enables the controller to be managed based on intentions. The policies are analyzed by the controller and compared with the observed data to provide insights and configurations for handling the volume of observability data.</p>"},{"location":"controller/docs/architecture/#task-flows","title":"Task flows","text":""},{"location":"controller/docs/architecture/#basic","title":"Basic","text":"<p>For basic use cases, the volume manager controller tasks can be configured as follows:</p> <pre><code>flowchart LR\n    direction TB\n    subgraph Ingress[\" \"]\n        direction TB\n        Ingress-Node[\"Ingest\\n(prometheus metrics)\"]\n    end\n    subgraph User[\" \"]\n        direction TB\n        User-Policy{{\"User\\nPolicy\"}}\n        User-Policy:::policyclass\n        User-Node[\"Rules\\nEngine\"]\n        User-Policy-.-&gt;User-Node\n    end\n    subgraph Insights[\" \"]\n        direction TB\n        Insights-Node[\"Volume reduction\\nInsights\"]\n    end   \n    subgraph Automation[\" \"]\n        direction TB\n        Automation-Policy{{\"Automation\\nPolicy\"}}\n        Automation-Policy:::policyclass\n        Automation-Node[\"Configuration\\nGenerator\"]\n        Automation-Policy-.-&gt;Automation-Node\n    end\n    subgraph Egress[\" \"]\n        direction TB\n        Egress-Node[\"Egress\\n(Processor\\nConfiguration)\"]\n    end     \nclassDef policyclass fill:lightblue    \nIngress --&gt; User --&gt; Insights --&gt; Automation --&gt; Egress</code></pre> <p>Note: In this MVP controller pipeline. The configuration of the controller uses a subset of the available tasks.</p>"},{"location":"controller/docs/architecture/#advanced","title":"Advanced","text":"<p>In advanced use cases, the task pipeline can be extended to provide additional capabilities. For example:</p> <pre><code>flowchart LR\n    direction TB\n    subgraph MetricsIngress[\" \"]\n        direction TB\n        MetricsIngress-Node[\"Ingest\\n(prometheus signals)\"]\n    end\n    subgraph LogsIngress[\" \"]\n        direction TB\n        LogsIngress-Node[\"Ingest\\n(logs signals)\"]\n    end    \n    subgraph Grouping[\" \"]\n        direction TB\n        Grouping-Policy{{\"Grouping\\nPolicy\"}}\n        Grouping-Policy:::policyclass\n        Grouping-Node[\"Signal-Grouping\\n(labels)\"]\n        Grouping-Policy-.-&gt;Grouping-Node        \n    end     subgraph Features[\" \"]\n        direction TB\n        Features-Node[\"Features-Extraction\\n(labels)\"]\n    end   \n\n    subgraph ObserFeatures[\" \"]\n        direction TB\n        ObserFeatures-Node[\"Observability-Domain\\nFeatures\\n(Extra labels)\"]\n    end      \n    subgraph User[\" \"]\n        direction TB\n        User-Policy{{\"User\\nPolicy\"}}\n        User-Policy:::policyclass\n        User-Node[\"Rules\\nEngine\"]\n        User-Policy-.-&gt;User-Node\n    end\n    subgraph Insights[\" \"]\n        direction TB\n        Insights-Node[\"Volume reduction\\nInsights\"]\n    end   \n    subgraph Automation[\" \"]\n        direction TB\n        Automation-Policy{{\"Automation\\nPolicy\"}}\n        Automation-Policy:::policyclass\n        Automation-Node[\"Configuration\\nGenerator\"]\n        Automation-Policy-.-&gt;Automation-Node\n    end\n    subgraph Egress[\" \"]\n        direction TB\n        Egress-Node[\"Egress\\n(Processor\\nConfiguration)\"]\n    end     \nclassDef policyclass fill:lightblue    \nMetricsIngress --&gt; Grouping --&gt; Features --&gt; ObserFeatures --&gt; User --&gt; Insights --&gt; Automation --&gt; Egress\nLogsIngress --&gt; Grouping</code></pre>"},{"location":"controller/docs/architecture/#task-types","title":"Task types","text":"<p>Following are basic explanations for each of the task types:</p> Task Type Description Ingest Ingest tasks are responsible for ingesting information into the controller. The objects ingested into the controller are \u201csignals\u201d. Signals can be each of the basic common observability data sources: metrics, logs, traces, etc. Signals can be ingested into the controller both synchronously and asynchronously. Grouping Grouping tasks use data from the signals to cluster, group, and partition multiple signals into a <code>signal group</code>. The grouping rules are policy-driven and rely on meta-data and data from the signals. The clustering is  based on raw labels provided as part of the signals. Feature-Extraction Feature Extraction tasks are responsible for the basic statistical analysis of the signals. They analyze the signal behaviors and produce a basic set of understandings of the signals. Observability-Analysis Observability analysis tasks are responsible for domain-specific analysis of signals generating observability-level understandings of signals User-Policy-Analyzer The policy enforcer will intersect the user-provided policies with the observability signals information and analysis gathered by the controller to generate policy driver information. System-Policy-Analyzer system policy analyzer tasks are responsible for the analysis of system behavior and the correlation of system risk analysis with the observability data. The  tasks will annotate the signals with relevant information to identify the dynamic applicability of the signals according to the policies Signal-Insight Insight tasks are responsible for the generation of volume management insights. Those insights are user-facing outputs of the pipelines and according to policies can be consumed by the users or pushed. The insights are tangible, environment-specific, and dynamic. They provide insights into volume management behaviors and action recommendations. Automation/configuration generator Automation tasks are responsible for the generation of per-processor configuration based on the action recommendation of the insights tasks. The configurations are sent using a customer-provided control plane to the processors to enforce the volume management reductions automatically. <p>TBD:</p> <p>Configurations the configuration tasks are</p> <p>Feature extraction feature</p> <p>Persistence:</p> <p>Objects to consider:</p>"},{"location":"controller/docs/config/","title":"Config","text":"<p>Configuration of the pipeline stages is implemented via a yaml file. The 'pipeline' section of the yaml file specifies the order in which to run the stages. The 'parameters' section of the yaml file specifies the particular parameters for each of the stages. Each stage has a name, and the names must match between the 'pipeline' and 'parameters' sections.</p> <p>Standard field names for each stage are: - name - type (e.g. ingest, extract, insights) - subtype (e.g. file ingest, promql ingest) - input_data (list of input (lists)) - output_data (list of output (lists)) - config (configuration specific to this stage)</p> <p>A sample config file might look like this:</p> <pre><code>pipeline:\n- name: ingest_stage\n- name: extract_stage\n  follows: [ingest_stage]\n- name: insights_stage\n  follows: [extract_stage]\nparameters:\n- name: ingest_stage\n  type: ingest\n  subtype: file\n  input_data: []\n  output_data: [signals]\n  config:\n    file_name: &lt;../some_file.json&gt;\n- name: extract_stage\n  type: extract\n  subtype: &lt;some extract subtype&gt;\n  input_data: [signals]\n  output_data: [extracted_signals]\n  config:\n- name: insights_stage\n  type: insights\n  subtype: &lt;some insights subtype&gt;\n  input_data: [extracted_signals]\n  output_data: [text_insights]\n  config:\n</code></pre> <p>A stage may follow multiple other stages, and may receive input from multiple earlier stages.</p>"},{"location":"controller/docs/config/#data-types","title":"Data Types","text":""},{"location":"controller/docs/config/#time-series","title":"Time Series","text":"<p>Time Series data is typically provided using the prometheus model with the following fields: <pre><code>{\n  \"status\": \"success\",\n  \"data\": {\n    \"resultType\": \"matrix\",\n    \"result\": [\n                {\n                    \"metric\": {\n                        \"__name__\": \"commit_latency_ms\",\n                        \"instance\": \"02:00:06:ff:fe:4b:b5:ac\",\n                        \"plugin\": \"ceph\",\n                        \"label\": \"ceph\",\n                        \"snapshotId\": \"T9Arslj9-1Y3UYv6biDGDizVqzI\",\n                        \"job\": \"prometheus\"\n                        },\n                    \"values\": [\n                        [1715252491.0, 0.0], \n                        [1715252492.0, 0.0], \n                        ....\n                        ]\n                }\n                .....\n              ]\n    }\n}\n</code></pre></p> <p>The <code>metric</code> field contains various metadata, including the <code>__name__</code> of the metric being reported; other fields are optional. The time series data are contained in the <code>values</code> field as a list of <code>&lt;timestamp, value&gt;</code> ordered pairs.</p>"},{"location":"controller/docs/config/#signal","title":"Signal","text":"<p><code>Signal</code> is an internal data type used to store time-series data.  It stores the metadata provided in the <code>metric</code> field and the time-series provided in the <code>values</code> field.</p>"},{"location":"controller/docs/config/#signals","title":"Signals","text":"<p><code>Signals</code> is an internal data type used to store multiple objects of type <code>Signal</code>. It has some internally defined metadata plus a list of <code>Signal</code> structures.</p>"},{"location":"controller/docs/config/#details-about-some-types-of-stages","title":"Details about some types of stages","text":""},{"location":"controller/docs/config/#ingest","title":"Ingest","text":"<p>An <code>ingest</code>-type stage typically reads data from some external source. The details of the external source (file location, url, security parameters) are provided in the <code>config</code> section of the stage parameters. An <code>ingest</code> stage is expected to have no <code>input_data</code> (<code>input_data: []</code>). An <code>ingest</code> type stage outputs a list containing a single element (of type <code>Signals</code>).</p>"},{"location":"controller/docs/config/#extract","title":"Extract","text":"<p>An <code>extract</code>-type stage typically performs some kind of transformation or metadata generation on <code>Signals</code>. The <code>input_data</code> should contain a single <code>Signals</code> element and the <code>output_data</code> should contain a single <code>Signals</code> element. For example: <pre><code>- name: feature_extraction_tsfel\n  type: extract\n  subtype: tsfel\n  input_data: [classified_signals]\n  output_data: [extracted_signals]\n</code></pre></p>"},{"location":"controller/docs/config/#map-reduce","title":"Map Reduce","text":"<p>A map_reduce stage takes some input, breaks it up into some number of pieces, and then runs some computation (possibly in parallel) on each of the pieces. The output of the <code>map</code> operation is a list of sublists. Each sublist is provided to a different instance of the <code>compute</code>. The <code>compute</code> part of a map_reduce stage takes a single sublist and outputs a single sublist. The outputs of each of the computations are then collected by the <code>reduce</code> operation into a combined output. The config of a map_reduce stage looks like the following:</p> <pre><code>- name: parallel_extract_stage\n  type: map_reduce\n  input_data: [signals]\n  output_data: [extracted_signals]\n  config:\n    map_function:\n      name: map1\n      type: map\n      subtype: simple\n      config:\n        &lt;any parameters needed to customize the map operation&gt;\n    compute_function:\n      name: extract_in_parallel\n      type: extract\n      subtype: tsfel\n      config:\n        &lt;any parameters needed to customize the compute operation&gt;\n    reduce_function:\n      name: reduce1\n      type: reduce\n      subtype: simple\n      config:\n        &lt;any parameters needed to customize the reduce operation&gt;\n</code></pre> <p>All map_reduce <code>compute</code> operations are of the same structure. A single list as input (of type Signals) and a single list as output (of type Signals). These must be preregistered in the code base as valid map_reduce <code>compute</code> operations. The <code>map</code> and <code>reduce</code> operations must likewise be preregistered in the code base as such operations.</p> <p>The map_reduce operation may be performed in parallel on multiple processes. This is achieved by specifying an additional configuration parameter under global_settings.</p> <pre><code>global_settings:\n  number_of_workers: 8\n</code></pre>"},{"location":"controller/docs/config/#insights","title":"Insights","text":"<p>An <code>insights</code> stage performs the analytics on the data. The <code>input_data</code> should contain a single <code>Signals</code> element (typically after feature extraction) and the <code>output_data</code>  contains 3 lists: [signals_to_keep, signals_to_reduce, text_insights].</p> <p>The following insights are currently supported: - zero_values: identify signals that have constant zero value. - fixed_values: identify signals that have constant non-zero value. - monotonic: identify signals that are monotonic (typical of a counter) - pairwise_correlations: identify signals that are directly correlated to some other signal. - compound_correlations: identify signals that are some linear combination of other signals.</p> <p>The order of the computed analytics should be specified. It is expected that typically the zero-valued and fixed-valued signals are identified, and then the remaining signals are tested for monotonicity or correlation.</p> <p>The zero-valued, fixed-valued, and correlated signals are primary candidates to be dropped. The monotonic signals are primary candidates to have their measurement frequency reduced.</p> <p>The format of the <code>insights</code> stage is as follows: <pre><code>- name: generate_insights\n  type: insights\n  subtype:\n  input_data: [extracted_signals]\n  output_data: [signals_to_keep, signals_to_reduce, text_insights]\n  config:\n    analysis_chain:\n    - type: zero_values\n      close_to_zero_threshold: 0.02\n    - type: fixed_values\n      filter_signals_by_tags: [\"zero_values\"]\n    - type: monotonic\n      filter_signals_by_tags: [\"zero_values\",\"fixed_values\"]\n    - type: pairwise_correlations\n      filter_signals_by_tags: [\"zero_values\",\"fixed_values\"]\n      pairwise_similarity_threshold: 0.02\n      pairwise_similarity_distance_method: pearson\n    - type: compound_correlations\n      filter_signals_by_tags: [\"zero_values\",\"fixed_values\",\"pairwise_correlations\"]\n      compound_similarity_threshold: 1\n</code></pre></p> <p>Any subset of the available analytics may be specified in the <code>analytics_chain</code>.</p> <p>The <code>filter_signals_by_tags</code> key specifies which signals to exclude from the particular analysis stage. Thus, we do not include the zero-valued and fixed-valued signals in the pairwise-correlation analytic. Additional parameters are availabe for some of the analytics.</p> <p>These insights can be viewed in the <code>controller</code> gui (in the demo see http://localhost:5000/insights).</p>"},{"location":"controller/docs/config/#config_generator","title":"Config_generator","text":"<p>The <code>config_generator</code> stage takes the data proviced by the <code>insights</code> stage and produces yaml files to configure the metrics collector to utilize the inisights. The <code>input_data</code> should contain 3 lists of signals: [extracted_signals, signals_to_keep, signals_to_reduce].</p> <p>The format of the <code>config_generator</code> stage is as follows: <pre><code>- name: config_generator_processor\n  type: config_generator\n  subtype: pmf_processor\n  input_data: [extracted_signals, signals_to_keep, signals_to_reduce]\n  output_data: [r_value]\n  config:\n    signal_filter_reduce_template: \"k8s_|nwdaf_|process_\"\n    signal_filter_adjust_template: \"k8s_|nwdaf_|process_\"\n    signal_name_template: \"$original_name\"\n    signal_condition_template: \"cluster=$cluster and instance=$instance\"\n    processor_id_template: \"$processor\"\n    directory: /tmp\n    url: http://manager:5010\n</code></pre></p> <p>The <code>signal_filter_reduce_template</code> parameter contains a regular expression of metric names that interest us. Metrics whose names do not match the <code>signal_filter_reduce_template</code> are ignored when computing the list of metrics to reduce. Similarly, metrics whose names do not match the <code>signal_filter_adjust_template</code> are ignored when computing the list of metrics whose sampling frequency to adjust. The <code>url</code> parameter specifies where to send the produced generated configurations. The <code>directory</code> parameter may be used to specify where to save (locally) a copy of the generated configuration files.</p> <p>We support both PMF (<code>subtype: pmf_processor</code>) and otel (<code>subtype: otel_processor</code>) formats.</p> <p>To specify a customized metric frequency for counters (when using otel), the following parameter can be used:</p> <pre><code>  config:\n    counter_default_interval: 30s\n</code></pre> <p>To specify a specific sampling frequency for specified metrics, the following config parameters may be used: <pre><code>  config:\n    metrics_adjustment:\n    - name_template: process_\n      interval: 20s\n    - name:_template k8s_pod_\n      interval: 10s\n</code></pre> These <code>name_template:</code> parameters match a regular expression of metric names. In this example, all metrics whose names contain <code>process_</code> will have their metrics reporting adjusted to every 20s.</p> <p>These can be combined into a configuration that looks like this: <pre><code>  config:\n    signal_filter_reduce_template: \"k8s_|nwdaf_|process_\"\n    signal_filter_adjust_template: \"k8s_|nwdaf_|process_\"\n    signal_name_template: \"$original_name\"\n    signal_condition_template: \"resource.attributes[\\\"processor\\\"] == \\\"$processor\\\" and resource.attributes[\\\"instance\\\"] == \\\"$instance\\\"\"\n    processor_id_template: \"$processor\"\n    counter_default_interval: 30s\n    metrics_adjustment:\n    - name_template: process_\n      interval: 20s\n    - name_template: k8s_\n      interval: 10s\n    - name_template: .*\n      tag_filter: [\"monotonic\"]\n      interval: 30s\n    directory: /tmp\n    url: http://manager:5010\n</code></pre> The <code>name_template: .*</code> matches all regular expressions, and may be used to catch all other metrics that are not explicitly specified earlier. The rules for <code>metrics_adjustment</code> are imposed on a first-match basis. The <code>tag_filter</code> parameter specifies a list of tags to be checked to restrict the application of the <code>metrics_adjustment</code> rule. In this example, only those signals that contain a tag <code>monotonic</code> will match this catch-all <code>name_template</code> and have their sampling frequency adjusted to 30 seconds.</p>"},{"location":"controller/docs/configuration_parameters/","title":"Configuration parameters","text":""},{"location":"controller/docs/configuration_parameters/#list-of-project-classes","title":"List of project classes","text":""},{"location":"controller/docs/configuration_parameters/#common.configuration_api","title":"<code>common.configuration_api</code>","text":""},{"location":"controller/docs/configuration_parameters/#common.configuration_api--controller-configuration","title":"Controller configuration","text":"<p>This document describes the configuration of the volume management controller component. The configuration is stored in a YAML file, typically <code>config.yaml</code>. The configuration is read by the controller component when it starts. The configuration is provided to the <code>controller</code> using the <code>-c</code> command line argument.</p> <p>Typical example usage:</p> <pre><code>`./controller -c config.yaml`\n</code></pre> <p>The configuration is organized into two areas.</p> <ol> <li>Pipeline definition - this area describes the pipeline stages and the relationship between stages.  It includes only high-level configuration that does not describe the functionality of the pipeline, just  provides names to stages, and describes the order of execution.</li> </ol> <p>For each stage, there is a named section, describing the functional behavior and  configuration of the stage.</p> <ol> <li>Stage configuration - For each <code>named stage</code>, describes the functionality of the stage using <code>Type</code> and <code>subType</code>.   Additional specific configuration parameters according to the functionality.</li> </ol>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.AnalysisChainProcess","title":"<code>AnalysisChainProcess</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for each of the analysis processes.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class AnalysisChainProcess(BaseModel):\n    \"\"\"\n    Configuration for each of the analysis processes.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    type: InsightsAnalysisChainType  # The type of analysis process\n    filter_signals_by_tags: Optional[List[str]] = []  # Filter signals to analyze by list of tags\n    close_to_zero_threshold: Optional[float] = 0  # Threshold for close to zero analysis\n    pairwise_similarity_threshold: Optional[float] = 0.95  # Threshold for pairwise similarity\n    pairwise_similarity_method: Optional[str] = (\n        GenerateInsightsType.INSIGHTS_SIMILARITY_METHOD_PEARSON.value)  # Method for pairwise similarity\n    # The distance algorithm to use (scipy.spacial.distance) when using distance method\n    pairwise_similarity_distance_method: Optional[str] = \"\"\n    compound_similarity_threshold: Optional[float] = 0.99  # Threshold for compound similarity\n    access_log_file: Optional[str] = None  # Access_log file\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.BaseStageParameters","title":"<code>BaseStageParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Stage parameters.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class BaseStageParameters(BaseModel):\n    \"\"\"\n    Configuration for Stage parameters.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    name: str  # Name of stage instance\n    type: str  # Type of stage instance (e.g. ingest, extract, insights, etc)\n    subtype: Optional[str] = None  # Subtype of stage instance (e.g. file_ingest vs promql ingest)\n    input_data: Optional[List[str]] = []  # List of input data names for the stage\n    output_data: Optional[List[str]] = []  # List of output data names for the stage\n    cache_directory: Optional[str] = None  # Directory to store output data\n    config: Optional[dict] = {}  # Stage-specific configuration parameters\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.BaseStageSchedule","title":"<code>BaseStageSchedule</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for order of stages that make up the pipeline.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class BaseStageSchedule(BaseModel):\n    \"\"\"\n    Configuration for order of stages that make up the pipeline.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    name: str  # Name of stage instance\n    follows: Optional[List[str]] = []  # List of stages that must precede currently defined sage in the pipeline\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.ConfigGeneratorProcessor","title":"<code>ConfigGeneratorProcessor</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for processor-based configuration generation.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class ConfigGeneratorProcessor(BaseModel):\n    \"\"\"\n    Configuration for processor-based configuration generation.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    processor_id_template: Optional[str] = \"\"  # Template for processor ID\n    signal_name_template: Optional[str] = \"\"  # Template for signal name\n    # Template for signal condition\n    signal_condition_template: Optional[str] = \"\"\n    signal_filter_reduce_template: Optional[str] = \"\"  # Template for signal filter for dropped signals\n    signal_filter_adjust_template: Optional[str] = \"\"  # Template for signal filter for adjusted signals\n    counter_default_interval: Optional[str] = \"\"  # time interval for counters\n    metrics_adjustment: Optional[List[FrequencyDef]] = []\n    directory: Optional[str] = None  # Directory to store configuration\n    url: Optional[str] = None  # URL to fetch data from\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.ConfigGeneratorSubType","title":"<code>ConfigGeneratorSubType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerates different subtypes for configuration generation.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class ConfigGeneratorSubType(Enum):\n    \"\"\"\n    Enumerates different subtypes for configuration generation.\n    \"\"\"\n    PIPELINE_CONFIG_GENERATOR_NONE = \"none\"\n    PIPELINE_CONFIG_GENERATOR_OTEL = \"otel\"\n    PIPELINE_CONFIG_GENERATOR_OTEL_PROCESSOR = \"otel_processor\"\n    PIPELINE_CONFIG_GENERATOR_PMF_PROCESSOR = \"pmf_processor\"\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.EncodeSerialized","title":"<code>EncodeSerialized</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.EncodeSerialized--configuration-for-serialized-file-encoding","title":"Configuration for serialized file encoding.","text":"<p>This configuration is applied when <code>stage</code>:   type: encode   subtype: serialized</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class EncodeSerialized(BaseModel):\n    \"\"\"\n    ### Configuration for serialized file encoding.\n    This configuration is applied when `stage`:\n      type: encode\n      subtype: serialized\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')\n    file_name: str  # Name of the file to ingest\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.EncodeSubType","title":"<code>EncodeSubType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerates different subtypes for encoding.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class EncodeSubType(Enum):\n    \"\"\"\n    Enumerates different subtypes for encoding.\n    \"\"\"\n    PIPELINE_ENCODE_SERIALIZED = \"serialized\"\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.ExtractSubType","title":"<code>ExtractSubType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerates different subtypes for metadata extraction.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class ExtractSubType(Enum):\n    \"\"\"\n    Enumerates different subtypes for metadata extraction.\n    \"\"\"\n    PIPELINE_EXTRACT_TSFEL = \"tsfel\"\n    PIPELINE_EXTRACT_TRIM = \"trim\"  # trim time series\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.FeatureExtractionTsfel","title":"<code>FeatureExtractionTsfel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for feature extraction using TSFEL.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class FeatureExtractionTsfel(BaseModel):\n    \"\"\"\n    Configuration for feature extraction using TSFEL.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    features_json_file: Optional[str] = \\\n        \"extract/tsfel_conf/limited_statistical.json\"  # JSON file for features\n    resample_rate: Optional[str] = \"30s\"  # Resampling rate\n    sampling_frequency: Optional[float] = (1 / 30)  # Sampling frequency\n    trim: Optional[bool] = False\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.FrequencyDef","title":"<code>FrequencyDef</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Placeholder configuration for no specific generation task.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class FrequencyDef(BaseModel):\n    \"\"\"\n    Placeholder configuration for no specific generation task.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    name_template: str  # (regex of) Name of metric for which to adjust frequency\n    tag_filter: Optional[List[str]] = []  # metadata tags to match\n    interval: str  # Time of specified interval\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.GenerateInsights","title":"<code>GenerateInsights</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for generating insights.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class GenerateInsights(BaseModel):\n    \"\"\"\n    Configuration for generating insights.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    # chain of analysis processes to be executed to generate insights\n    analysis_chain: List[AnalysisChainProcess] = [\n        AnalysisChainProcess(\n            type=InsightsAnalysisChainType.INSIGHTS_ANALYSIS_ZERO_VALUES.value),\n        AnalysisChainProcess(\n            type=InsightsAnalysisChainType.INSIGHTS_ANALYSIS_FIXED_VALUES.value,\n            filter_signals_by_tags=[InsightsAnalysisChainType.INSIGHTS_ANALYSIS_ZERO_VALUES.value]),\n        AnalysisChainProcess(\n            type=InsightsAnalysisChainType.INSIGHTS_ANALYSIS_PAIRWISE_CORRELATIONS.value,\n            filter_signals_by_tags=[InsightsAnalysisChainType.INSIGHTS_ANALYSIS_ZERO_VALUES.value,\n                                    InsightsAnalysisChainType.INSIGHTS_ANALYSIS_FIXED_VALUES.value]),\n        AnalysisChainProcess(\n            type=InsightsAnalysisChainType.INSIGHTS_ANALYSIS_COMPOUND_CORRELATIONS.value,\n            filter_signals_by_tags=[InsightsAnalysisChainType.INSIGHTS_ANALYSIS_ZERO_VALUES.value,\n                                    InsightsAnalysisChainType.INSIGHTS_ANALYSIS_FIXED_VALUES.value,\n                                    InsightsAnalysisChainType.INSIGHTS_ANALYSIS_PAIRWISE_CORRELATIONS.value]),\n        AnalysisChainProcess(\n            type=InsightsAnalysisChainType.INSIGHTS_ANALYSIS_METADATA_CLASSIFICATION.value),\n        AnalysisChainProcess(\n            type=InsightsAnalysisChainType.INSIGHTS_ANALYSIS_MONOTONIC.value,\n            filter_signals_by_tags=[InsightsAnalysisChainType.INSIGHTS_ANALYSIS_ZERO_VALUES.value,\n                                    InsightsAnalysisChainType.INSIGHTS_ANALYSIS_FIXED_VALUES.value]),\n    ]\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.GenerateInsightsType","title":"<code>GenerateInsightsType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerates different types of insights generation methods.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class GenerateInsightsType(Enum):\n    \"\"\"\n    Enumerates different types of insights generation methods.\n    \"\"\"\n    INSIGHTS_SIMILARITY_METHOD_PEARSON = \"pearson\"\n    INSIGHTS_SIMILARITY_METHOD_SPEARMAN = \"spearman\"\n    INSIGHTS_SIMILARITY_METHOD_KENDALL = \"kendall\"\n    INSIGHTS_SIMILARITY_METHOD_DISTANCE = \"distance\"\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.GeneratorNone","title":"<code>GeneratorNone</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Placeholder configuration for no specific generation task.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class GeneratorNone(BaseModel):\n    \"\"\"\n    Placeholder configuration for no specific generation task.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.GlobalSettings","title":"<code>GlobalSettings</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for global settings.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class GlobalSettings(BaseModel):\n    \"\"\"\n    Configuration for global settings.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    number_of_workers: Optional[int] = 0  # Number of processes to create to perform map-reduce operations in parallel\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.IngestDummy","title":"<code>IngestDummy</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for dummy ingestion.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class IngestDummy(BaseModel):\n    \"\"\"\n    Configuration for dummy ingestion.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.IngestFile","title":"<code>IngestFile</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.IngestFile--configuration-for-file-ingestion","title":"Configuration for file ingestion.","text":"<p>This configuration is applied when <code>stage</code>:   type: ingest   subtype: file</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class IngestFile(BaseModel):\n    \"\"\"\n    ### Configuration for file ingestion.\n    This configuration is applied when `stage`:\n      type: ingest\n      subtype: file\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')\n    file_name: str  # Name of the file to ingest\n    filter_metadata: Optional[str] = \"\"  # Metadata filter\n    ingest_name_template: Optional[str] = \"\"  # Template for ingest names\n    format: Optional[str] = IngestFormat.PIPELINE_INGEST_FORMAT_PROM.value\n    time_unit: Optional[str] = IngestTimeUnit.PIPELINE_TIME_UNIT_SECOND.value\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.IngestFormat","title":"<code>IngestFormat</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerates different subtypes for ingestion.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class IngestFormat(Enum):\n    \"\"\"\n    Enumerates different subtypes for ingestion.\n    \"\"\"\n    PIPELINE_INGEST_FORMAT_PROM = \"prometheus\"\n    PIPELINE_INGEST_FORMAT_INSTANA_INFRA = \"instana_infra\"\n    PIPELINE_INGEST_FORMAT_INSTANA_APP = \"instana_app\"\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.IngestPromql","title":"<code>IngestPromql</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for PromQL ingestion.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class IngestPromql(BaseModel):\n    \"\"\"\n    Configuration for PromQL ingestion.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    url: str  # URL to fetch data from\n    ingest_window: str  # Time interval for ingestion\n    filter_metadata: Optional[str] = \"\"  # Metadata filter\n    ingest_name_template: Optional[str] = \"\"  # Template for ingest names\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.IngestSerialized","title":"<code>IngestSerialized</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.IngestSerialized--configuration-for-serialized-file-ingestion","title":"Configuration for serialized file ingestion.","text":"<p>This configuration is applied when <code>stage</code>:   type: ingest   subtype: serialized</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class IngestSerialized(BaseModel):\n    \"\"\"\n    ### Configuration for serialized file ingestion.\n    This configuration is applied when `stage`:\n      type: ingest\n      subtype: serialized\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')\n    file_name: str  # Name of the file to ingest\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.IngestSubType","title":"<code>IngestSubType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerates different subtypes for ingestion.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class IngestSubType(Enum):\n    \"\"\"\n    Enumerates different subtypes for ingestion.\n    \"\"\"\n    PIPELINE_INGEST_DUMMY = \"dummy\"\n    PIPELINE_INGEST_FILE = \"file\"\n    PIPELINE_INGEST_PROMQL = \"promql\"\n    PIPELINE_INGEST_SERIALIZED = \"serialized\"\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.IngestTimeUnit","title":"<code>IngestTimeUnit</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerates different time units supported for time series data</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class IngestTimeUnit(Enum):\n    \"\"\"\n    Enumerates different time units supported for time series data\n    \"\"\"\n    PIPELINE_TIME_UNIT_SECOND = \"s\"\n    PIPELINE_TIME_UNIT_MILLISECOND = \"ms\"\n    PIPELINE_TIME_UNIT_MICROSECOND = \"us\"\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.InsightsAnalysisChainType","title":"<code>InsightsAnalysisChainType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerates analysis processes (used by insights analysis_chain)</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class InsightsAnalysisChainType(Enum):\n    \"\"\"\n    Enumerates analysis processes (used by insights analysis_chain)\n    \"\"\"\n    INSIGHTS_ANALYSIS_ZERO_VALUES = \"zero_values\"\n    INSIGHTS_ANALYSIS_FIXED_VALUES = \"fixed_values\"\n    INSIGHTS_ANALYSIS_MONOTONIC = \"monotonic\"\n    INSIGHTS_ANALYSIS_PAIRWISE_CORRELATIONS = \"pairwise_correlations\"\n    INSIGHTS_ANALYSIS_COMPOUND_CORRELATIONS = \"compound_correlations\"\n    INSIGHTS_ANALYSIS_METADATA_CLASSIFICATION = \"metadata_classification\"\n    INSIGHTS_ANALYSIS_ACCESS_LOG_INTERSECT = \"access_log_intersect\"\n    INSIGHTS_ANALYSIS_ACCESS_LOG_INTERSECT_NOT_ACCESSED = \"access_log_intersect_not_accessed\"\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.MapByName","title":"<code>MapByName</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for map operations by name pattern.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class MapByName(BaseModel):\n    \"\"\"\n    Configuration for map operations by name pattern.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    name_pattern: str  # Pattern for mapping by name\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.MapSimple","title":"<code>MapSimple</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for simple map operations.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class MapSimple(BaseModel):\n    \"\"\"\n    Configuration for simple map operations.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    number: int  # Number for mapping\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.MapSubType","title":"<code>MapSubType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerates different subtypes for map operations.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class MapSubType(Enum):\n    \"\"\"\n    Enumerates different subtypes for map operations.\n    \"\"\"\n    PIPELINE_MAP_SIMPLE = \"simple\"\n    PIPELINE_MAP_BY_NAME = \"by_name\"\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.MetadataClassificationFewShot","title":"<code>MetadataClassificationFewShot</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for few-shot metadata classification.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class MetadataClassificationFewShot(BaseModel):\n    \"\"\"\n    Configuration for few-shot metadata classification.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    # Pre-trained model to use\n    base_model: Optional[str] = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n    few_shot_classification_file: Optional[str] = (\n        \"./metadata_classification/data/observability_metrics_classification_zero_shot.json\"\n    )  # File containing few-shot classification external data\n    few_shot_pretrained_model_directory: Optional[str] = (\n        \"./metadata_classification/data/few_shot_pretrained_model\"\n    )  # Directory containing a pretrained few-shot model\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.MetadataClassificationRegEx","title":"<code>MetadataClassificationRegEx</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for regex metadata classification.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class MetadataClassificationRegEx(BaseModel):\n    \"\"\"\n    Configuration for regex metadata classification.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    regex_classification_file: Optional[str] = (\n        \"./metadata_classification/data/observability_metrics_classification_regex.json\"\n    )  # File containing regex classification external data\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.MetadataClassificationSubType","title":"<code>MetadataClassificationSubType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerates subtypes for metadata classification.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class MetadataClassificationSubType(Enum):\n    \"\"\"\n    Enumerates subtypes for metadata classification.\n    \"\"\"\n    # `metadata_classification_regex`: uses regex to perform metadata classification\n    PIPELINE_METADATA_CLASSIFICATION_REGEX = \"metadata_classification_regex\"\n    # `metadata_classification_zero_shot`: uses zero-shot (AI) technique to perform metadata classification\n    PIPELINE_METADATA_CLASSIFICATION_ZERO_SHOT = \"metadata_classification_zero_shot\"\n    # `metadata_classification_few_shot`: uses few-shot (AI) technique to perform metadata classification\n    PIPELINE_METADATA_CLASSIFICATION_FEW_SHOT = \"metadata_classification_few_shot\"\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.MetadataClassificationZeroShot","title":"<code>MetadataClassificationZeroShot</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for zero-shot metadata classification.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class MetadataClassificationZeroShot(BaseModel):\n    \"\"\"\n    Configuration for zero-shot metadata classification.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n    model: Optional[str] = \"roberta-large-mnli\"  # Pre-trained model to use\n    zero_shot_classification_file: Optional[str] = (\n        \"./metadata_classification/data/observability_metrics_classification_zero_shot.json\"\n    )  # File containing zero-shot classification external data\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.PipelineDefinition","title":"<code>PipelineDefinition</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for pipeline definition.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class PipelineDefinition(BaseModel):\n    \"\"\"\n    Configuration for pipeline definition.\n    \"\"\"\n    global_settings: Optional[dict] = {}\n    pipeline: List[BaseStageSchedule]  # Order of stages that make up the pipeline\n    parameters: List[BaseStageParameters]  # Specific parameters to configure each stage in the pipeline\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.ReduceSimple","title":"<code>ReduceSimple</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for simple reduce operations.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class ReduceSimple(BaseModel):\n    \"\"\"\n    Configuration for simple reduce operations.\n    \"\"\"\n    model_config = ConfigDict(extra='forbid')  # Configuration for the model\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.ReduceSubType","title":"<code>ReduceSubType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumerates different subtypes for reduce operations.</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class ReduceSubType(Enum):\n    \"\"\"\n    Enumerates different subtypes for reduce operations.\n    \"\"\"\n    PIPELINE_REDUCE_SIMPLE = \"simple\"\n</code></pre>"},{"location":"controller/docs/configuration_parameters/#common.configuration_api.StageType","title":"<code>StageType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Stage <code>type</code> (stage functionality): Each <code>named stage configuration</code> includes one of the following <code>type</code> (string) options:</p> Source code in <code>controller/common/configuration_api.py</code> <pre><code>class StageType(Enum):\n    \"\"\"\n    Stage `type` (stage functionality):\n    Each `named stage configuration` includes one of the following `type` (string) options:\n    \"\"\"\n    INGEST = \"ingest\"  # `ingest`: ingest data from various sources into the controller\n    # `extract`: performs feature extraction on the signals\n    EXTRACT = \"extract\"\n    INSIGHTS = \"insights\"  # `insights`: generates insights (analytics)\n    # `config_generator`: Generates and apply processor configurations\n    CONFIG_GENERATOR = \"config_generator\"\n    # `metadata_classification`: Metadata classification\n    METADATA_CLASSIFICATION = \"metadata_classification\"\n    # `map_reduce`: apply map reduce operations (for stages scalability)\n    MAP_REDUCE = \"map_reduce\"\n    ENCODE = \"encode\"  # `encode`: encode data\n</code></pre>"},{"location":"controller/docs/api/api/","title":"API","text":""},{"location":"manager/alertmanager/","title":"AlertManager","text":""},{"location":"manager/alertmanager/#local-testing","title":"Local Testing","text":"<ul> <li>Run metricgen, PMF, prometheus, thanos ingestor and query as described in processor readme.</li> <li>With current release of thanos latest alert manager version supported is v 0.25.0  since later than that use v2 api not supported by thanos yet</li> <li>Start alert manager <code>./alertmanager --config.file=alertmanager.yml</code></li> <li>Start thanos rule <code>/thanos rule --grpc-address=0.0.0.0:30901 --query=0.0.0.0:19192 --http-address=\"0.0.0.0:10903\" --rule-file=rules.yml --alertmanagers.url=http://127.0.0.1:9093</code></li> <li>To test alertmanager receiver start dummy http server <code>python3 -m http.server 5001</code></li> <li>To note intevals and wait_time need to be updated as per notification requirement.</li> <li>To dynamically update rules you need to update the file and trigger reload of it in thanos ruler. e.g. using curl: <code>curl --request POST 0.0.0.0:10903/-/reload</code> (<code>http-address</code> of ruler)</li> <li>UI: <code>0.0.0.0:10903</code> to view triggered alerts</li> </ul>"},{"location":"manager/alertmanager/#apis-for-rules","title":"APIs for rules","text":"<ul> <li>/add/\\&lt;rule_id&gt; Add alert rule (if the rule id entry already exists it will replace it)</li> <li>/delete/\\&lt;rule_id&gt; Delete alert rule</li> <li>/\\&lt;rule_id&gt; Get details of an alert rule</li> <li>/rules Get details of all the rules</li> </ul>"},{"location":"manager/alertmanager/#example-curls","title":"Example CURLs","text":"<ul> <li>Add rule abc <code>curl -X POST --data-binary @request.yml -H \"Content-type: text/x-yaml\" 0.0.0.0:8090/add/abc</code></li> <li>Delete rule abc <code>curl  0.0.0.0:8090/delete/abc</code></li> <li>Get details of rule abc <code>curl  0.0.0.0:8090/abc</code></li> <li>Get details of all the rules <code>curl  0.0.0.0:8090/rules</code></li> </ul>"},{"location":"manager/configurator/","title":"Manager","text":"<p>Manager: is deployed at the central cloud location and is responsible for managing observability data transformations at the edge locations. It is a user-facing component with a REST interface to create/update/delete the rules that define the transformation DAGs to be enabled on a certain edge site(s) when some condition is met. The user can also specify default transformation DAGs for edge sites. The Manager coordinates with the processors to enforce transformation DAGs based on user-defined rules when conditions are satisfied.</p>"},{"location":"manager/configurator/#testing-locally","title":"Testing Locally","text":"<ol> <li> <p>Specify the config file. A sample config YAML is present in config/config.yaml. You need to specify the path to it using an environment variable. For example:    <pre><code>export CONFIG_FILE=/path/to/your/config/config.yaml\n</code></pre></p> </li> <li> <p>Install all dependencies using requirements.txt:    <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Run the application:    <pre><code>python3 main.py\n</code></pre></p> </li> </ol> <p>Once the application is running, it is accessible at <code>http://localhost:5010</code>.</p>"},{"location":"manager/configurator/#processor-configuration-apis","title":"Processor Configuration APIs","text":"<p>All processor configuration APIs are available at <code>http://localhost:5010/api/v1/processor_config</code>.</p>"},{"location":"manager/configurator/#rules-apis","title":"Rules APIs","text":"<p>All rules APIs are available at <code>http://localhost:5010/api/v1/rules</code>.</p>"},{"location":"manager/configurator/#swagger-ui","title":"Swagger UI","text":"<p>The Swagger UI is accessible at <code>http://localhost::5010/apidocs/</code>.</p>"},{"location":"manager/configurator/docs/configuration_parameters/","title":"Configuration","text":""},{"location":"manager/configurator/docs/configuration_parameters/#list-of-project-classes","title":"List of project classes","text":""},{"location":"manager/configurator/docs/configuration_parameters/#models.ProcessorsConfig","title":"<code>models.ProcessorsConfig</code>","text":""},{"location":"otel-collector-proxy/","title":"oTel-collector-proxy","text":"<p>The oTel collector proxy component is the Edge proxy allowing OVM Manager to control oTel collectors and allow dynamic configuration of oTel processors as part of the collectors configuration.</p> <p>The oTel collector proxy is deployed on each Edge location. The following functionalities are supported by the proxy:</p> <ol> <li> <p>Communicates with OVM Manager over the control place - This communication is for configuration to be  applied by the proxy to the oTel collector. The configuraiton is applied as a configuraiton file that is  shared with the relavant oTel controller.</p> </li> <li> <p>Configuration transformation - The OVM Manager confiration semantics is translated into oTel processor configuration and saved into the configuration file</p> </li> <li> <p>oTel collector update - As part of the oTel configuration update, the proxy performs a restart to the collector to enforce the configuration changes (a limitation of the oTel collector) </p> </li> </ol>"},{"location":"otel-collector-proxy/#usage","title":"Usage","text":"<p>The oTel collector is configured using command line argument:</p> <pre><code>python rest_server.py --help\nusage: rest_server.py [-h] --save-update-path SAVE_UPDATE_PATH --processor-file-to-update PROCESSOR_FILE_TO_UPDATE\n                      --docker-container-reset-on-update DOCKER_CONTAINER_RESET_ON_UPDATE\n\nFlask JSON Upload Server\n\noptions:\n  -h, --help            show this help message and exit\n  --save-update-path SAVE_UPDATE_PATH\n                        Path where the updated YAML file will be saved\n  --processor-file-to-update PROCESSOR_FILE_TO_UPDATE\n                        oTel collector YAML file path - to be updated\n  --docker-container-reset-on-update DOCKER_CONTAINER_RESET_ON_UPDATE\n                        The name of the Docker container to reset on update\n</code></pre>"},{"location":"otel-collector-proxy/#development","title":"Development","text":"<p>A Makefile is provided for basic operations:</p> <pre><code>$ make\n\nUsage:\n  make &lt;target&gt;\n  help                  Display this help.\n\nDocker\n  docker_build          build to container\n  docker_push           push to docker registry\n\nDevelopment\n  run                   Execute the code\n  tests                 Execute tests\n\nCI\n  ci_push               Executed upon ci push (merge) event\n</code></pre>"},{"location":"processor/","title":"Description of Architecture","text":"<p>There are 3 main components which make up the processor.  1. <code>transformer</code> 2. <code>morpher</code> 3. <code>dbinterface</code></p> <p>Additionally, we have a component <code>benchmarker</code> which is used to measure and store timings of each request at each component.</p>"},{"location":"processor/#transformer","title":"Transformer","text":"<p>The transformer is a HTTP proxy which receives metrics in the Prometheus Remote Write format, calls a Transform function on the data and sends metrics in the Remote Write format.</p> <p>It receives HTTP POST payloads containing metric data (encoded as Protobuf) from a <code>listen</code> address, converts it into the <code>prompb</code> format to perform processing on the metrics (through the Transform function) and finally encodes the <code>prompb</code> back to HTTP payload and sends it to the <code>target</code> address. The <code>listen</code> and <code>target</code> address are set through the processor's flags.</p>"},{"location":"processor/#morpher","title":"Morpher","text":"<p>The morpher manages the DAG chain and the compilation of the DAG from the user submitted format to the SQL query that needs to be executed. Users represent their processor DAG (more specifically a tree) as a yaml which is then compiled to build a set of interlinked SQL queries which use the previous DAG node output as input.</p> <p>The morpher receiver exposes 2 endpoints, <code>create</code> and <code>delete</code> to update the DAG, which are described below. It further parses the selector and parameters before passing it to the morpher.</p>"},{"location":"processor/#dbinterface","title":"DBInterface","text":"<p>The <code>dbinterface</code> mainly deals with managing the Database throughout the processor's lifecycle by inserting metrics into the db, executing the SQL commands corresponding to the DAG and finally moving metrics which need to be exported, from the database.</p> <p>SQLite offers 2 modes, file mode and in-memory mode for managing the DB.</p>"},{"location":"processor/#processor-api-endpoints","title":"Processor API Endpoints","text":"<p>The processor exposes 2 endpoints to update DAGs dynamically at the <code>8100</code> port. This is the <code>/morphchain</code> endpoint with the <code>/create</code> and <code>/delete</code> endpoints for creating and deleting DAGs respectively.</p> <p>To create a DAG, call the <code>create</code> endpoint with the DAG yaml as given in example <code>../contrib/processor_dag_examples/filter.yaml</code></p> <pre><code>curl IP:8100/morphchain/create --data-binary '@controller/dag_examples/filter.yaml'\n</code></pre> <p>To reset the DAG, call the <code>delete</code> endpoint. This will reset to the default DAG, which sends all metrics at 30 sec frequency.</p> <pre><code>curl IP:8100/morphchain/delete\n</code></pre>"},{"location":"processor/#running-locally","title":"Running locally","text":"<p>The architecture has 4 main components which we need to bring up to run the end to end system locally: 1. Metric source: To generate metrics in the Prometheus format. While a tool like Avalanche can be used, it does not allow customization of the number and style of labels. We have built a custom generator which allows to customize the labels, in <code>ec-metric-gen</code>. 2. Local metric collector: A metric collector agent which collects metrics from multiple metric sources and exposes it in the Prometheus Remote Write format. This could be Prometheus or some other tool. For further steps, we consider Prometheus. 3. Metric processor: This is the current repo. Compilation and running instructions are given below. 4. Central Metric collector: Metric collector agent which can receive from multiple metric collectors (in the Prometheus Remote Write format). Some tools for this are Thanos, Cortex or even Prometheus. For further steps, we consider Thanos.</p> <p>Prometheus can be installed and run locally  or run in a docker container. Similarly, Thanos can be installed locally or run in a docker container.</p> <p>To compile the processor, run: <pre><code>go build main.go\n</code></pre></p> <p>Running: 1. Start metric generator - we use the <code>gen-metrics-gutentag.py</code> script. For more details on the different options it provides, check its README. It exposes port 8000 for Prometheus to scrape metrics.    <pre><code>$ cd ../contrib/ec-metric-gen/\n$ python3 gen-metrics-gutentag.py --fake --nmetrics 1000 --nlabels 10\n\nINFO:root:Generating 10 metrics excluding labels and 1000 metrics including labels\n</code></pre> 2. Start local metric collector. A sample Prometheus configuration is given here. There are 2 main configurations:</p> <pre><code>Setting the target to scrape from (the metric-gen prometheus scrape endpoint)\n\n```\nscrape_configs:\n  static_configs:\n    - targets: [\"localhost:8000\"]\n```\n\nand the push URL. This is the processor's transformer endpoint.\n\n```\nremote_write:\n  - url: \"http://localhost:8081/api/v1/receive\"\n```\n\nWe run prometheus as:\n\n```\n$ prometheus --config.file=\"../contrib/ec-metric-gen/prometheus.yml\"\n```\n\nUntil the processor starts, prometheus may complain about connection being refused. Once the processor starts, it will start forwarding the metrics.\n\n```\nts=2024-05-06T03:29:44.164Z caller=dedupe.go:112 component=remote level=warn remote_name=e5a93c url=http://localhost:8081/api/v1/push msg=\"Failed to send batch, retrying\" err=\"Post \\\"http://localhost:8081/api/v1/push\\\": dial tcp [::1]:8081: connect: connection refused\"\n```\n</code></pre> <ol> <li> <p>Start the PMF processor. It takes 2 flags, one for the target (this is the Thanos/Cortex endpoint) and one for the listen (this is the endpoint which listens for Prometheus Remote Write pusher).</p> <p>Target needs to be configured based on the Central Metric Collector. Thanos listens on port <code>19291</code> and Cortex listens on port <code>9090</code>.</p> <p><pre><code>$ go run main.go --target http://localhost:19291\" --listen \"0.0.0.0:8081\"\n</code></pre> 4. Start Thanos. Thanos has the receive and query components which need to start up: <pre><code>./thanos receive --grpc-address=0.0.0.0:10901 --remote-write.address=0.0.0.0:19291 --label=receive=\\\"true\\\"\n./thanos query --grpc-address=0.0.0.0:20901 --http-address=0.0.0.0:19192 --store=0.0.0.0:10901\n</code></pre></p> </li> </ol> <p>To visualize the metrics, you may additionally set up Grafana and connect to Thanos.</p>"}]}